{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression, Lasso, Ridge\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sn\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "import tqdm\n",
    "import copy\n",
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from rlace import solve_adv_game\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "from sklearn import neural_network\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import warnings\n",
    "import argparse\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "from rlace import init_classifier\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import rlace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios_data_path = \"../WWW/rlace/bios/data/bios_data\"\n",
    "bios_encodings_path = \"../WWW/rlace/bios/data/encodings/freezed\"\n",
    "\n",
    "data = []\n",
    "for mode in [\"train\", \"dev\", \"test\"]:\n",
    "    with open(bios_data_path + \"/\" + \"{}.pickle\".format(mode), \"rb\") as f:\n",
    "        data.append(pickle.load(f))\n",
    "        \n",
    "train,dev,test = data\n",
    "gender2ind = {\"m\": 0, \"f\": 1}\n",
    "train_profs, dev_profs = np.array([d[\"p\"] for d in train]), np.array([d[\"p\"] for d in dev])\n",
    "train_gender, dev_gender = np.array([gender2ind[d[\"g\"]] for d in train]), np.array([gender2ind[d[\"g\"]] for d in dev])\n",
    "\n",
    "\n",
    "data = []\n",
    "for mode in [\"train\", \"dev\", \"test\"]:\n",
    "    with open(bios_encodings_path + \"/\" + \"{}_cls.npy\".format(mode), \"rb\") as f:\n",
    "        data.append(np.load(f))\n",
    "        \n",
    "train_x, dev_x, test_x = data\n",
    "pca = PCA(n_components=300, random_state=0)\n",
    "pca.fit(train_x)\n",
    "train_x = pca.transform(train_x)\n",
    "dev_x = pca.transform(dev_x)\n",
    "test_x = pca.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "profs_counts = Counter(train_profs)\n",
    "common_profs = [p for p,c in profs_counts.most_common(10)]\n",
    "import itertools\n",
    "pairs = []\n",
    "for pair in itertools.product(common_profs, repeat=2):\n",
    "    if (pair[0], pair[1]) not in pairs and (pair[1], pair[0]) not in pairs and pair[0] != pair[1]:\n",
    "        pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "idx = list(range(len(pairs)))\n",
    "random.shuffle(idx)\n",
    "\n",
    "pairs_sample = [pairs[i] for i in idx[:15]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learn projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "num_iters = 60000\n",
    "rank=1\n",
    "optimizer_class = torch.optim.SGD\n",
    "optimizer_params_P = {\"lr\": 0.001, \"weight_decay\": 1e-5, \"momentum\":0.5}\n",
    "optimizer_params_predictor = {\"lr\": 0.001,\"weight_decay\": 1e-5, \"momentum\": 0.5}\n",
    "epsilon = 0.0025 # stop 0.25% from majority acc\n",
    "batch_size = 256\n",
    "rank=1\n",
    "device=\"cpu\"\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "for pair in tqdm.tqdm_notebook(pairs_sample):\n",
    "    print(pair)\n",
    "    with open(\"acl/data/{}-{}.pickle\".format(pair[0], pair[1]), \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        train_x, train_z, train_y = data[\"train\"]\n",
    "        dev_x, dev_z, dev_y = data[\"dev\"]\n",
    "        \n",
    "        output = solve_adv_game(train_x, train_z, dev_x, dev_z, rank=rank, device=device, out_iters=num_iters, optimizer_class=optimizer_class, optimizer_params_P =optimizer_params_P, optimizer_params_predictor=optimizer_params_predictor, epsilon=epsilon,batch_size=batch_size)\n",
    "        with open(\"acl/projs/{}-{}.pickle\".format(pair[0], pair[1]), \"wb\") as f:\n",
    "            pickle.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['score', 'P_before_svd', 'P'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train gender classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21743/900876923.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for pair in tqdm.tqdm_notebook(pairs_sample):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f5cfa4f09848b99b925705e75b1280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair2genderclf = defaultdict(list)\n",
    "pair2professionclf = defaultdict(list)\n",
    "pair2genderclf_original = defaultdict(list)\n",
    "pair2professionclf_original = defaultdict(list)\n",
    "\n",
    "num_clfs = 10\n",
    "\n",
    "for pair in tqdm.tqdm_notebook(pairs_sample):\n",
    "    with open(\"acl/data/{}-{}.pickle\".format(pair[0], pair[1]), \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        train_x, train_z, train_y = data[\"train\"]\n",
    "        dev_x, dev_z, dev_y = data[\"dev\"]\n",
    "        \n",
    "    with open(\"acl/projs/{}-{}.pickle\".format(pair[0], pair[1]), \"rb\") as f:\n",
    "        P = pickle.load(f)[\"P\"]\n",
    "\n",
    "    train_x_proj = train_x@P\n",
    "    pair_str = pair[0]+ \"-\" + pair[1]\n",
    "    dev_x_proj = dev_x@P\n",
    "\n",
    "    for i in range(num_clfs):\n",
    "        gender_clf = init_classifier()\n",
    "        gender_clf.fit(train_x_proj, train_z)\n",
    "        pair2genderclf[pair_str].append(gender_clf)\n",
    "        \n",
    "        prof_clf = init_classifier()\n",
    "        prof_clf.fit(train_x_proj, train_y)\n",
    "        pair2professionclf[pair_str].append(prof_clf)\n",
    "        \n",
    "        gender_clf = init_classifier()\n",
    "        gender_clf.fit(train_x, train_z)\n",
    "        pair2genderclf_original[pair_str].append(gender_clf)\n",
    "        \n",
    "        prof_clf = init_classifier()\n",
    "        prof_clf.fit(train_x, train_y)\n",
    "        pair2professionclf_original[pair_str].append(prof_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"acl/interim/pair2genderclf.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pair2genderclf, f)\n",
    "    \n",
    "with open(\"acl/interim/pair2profclf.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pair2professionclf, f)\n",
    "\n",
    "with open(\"acl/interim/pair2genderclf_original.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pair2genderclf_original, f)\n",
    "    \n",
    "with open(\"acl/interim/pair2profclf_original.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pair2professionclf_original, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gender_clf(pair, ind=0, original=False):\n",
    "\n",
    "    with open(\"acl/interim/pair2genderclf{}.pickle\".format(\"_original\" if original else \"\"), \"rb\") as f:\n",
    "        pair2genderclf = pickle.load(f)\n",
    "    return pair2genderclf[pair[0]+\"-\"+pair[1]][ind]\n",
    "             \n",
    "def load_prof_clf(pair, ind=0, original=False):\n",
    "\n",
    "    with open(\"acl/interim/pair2genderclf{}.pickle\".format(\"_original\" if original else \"\"), \"rb\") as f:\n",
    "        pair2profclf = pickle.load(f)\n",
    "    return pair2profclf[pair[0]+\"-\"+pair[1]][ind]\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train main-task classifier, evaluate V-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(y):\n",
    "    \n",
    "    counts = Counter(y)\n",
    "    fracts = [c/sum(counts.values()) for c in counts.values()]\n",
    "    return scipy.stats.entropy(fracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "professor-attorney 0 0.0008130818692644315\n",
      "hard  0.55 professor-attorney 0 -0.002193503156774246\n",
      "hard  0.6 professor-attorney 0 -0.0013127430028635478\n",
      "hard  0.7 professor-attorney 0 -5.8606770173885714e-05\n",
      "hard  0.8 professor-attorney 0 -0.0008075338696763579\n",
      "hard  0.9 professor-attorney 0 -0.0003585057574706507\n",
      "hard  0.99 professor-attorney 0 -0.0010201818975216215\n",
      "professor-attorney 1 -0.0002537870566589051\n",
      "hard  0.55 professor-attorney 1 -0.0004858139365414571\n",
      "hard  0.6 professor-attorney 1 -0.0001614055739520559\n",
      "hard  0.7 professor-attorney 1 7.611056328915566e-05\n",
      "hard  0.8 professor-attorney 1 -0.0002064685800031718\n",
      "hard  0.9 professor-attorney 1 -0.00044448571443855833\n",
      "hard  0.99 professor-attorney 1 -0.0004956620372327247\n",
      "professor-attorney 2 -0.00018721601070958993\n",
      "hard  0.55 professor-attorney 2 2.467539664019469e-05\n",
      "hard  0.6 professor-attorney 2 -0.00047493907741535946\n",
      "hard  0.7 professor-attorney 2 7.644594053624054e-05\n",
      "hard  0.8 professor-attorney 2 3.146408611465201e-05\n",
      "hard  0.9 professor-attorney 2 7.067845160735331e-05\n",
      "hard  0.99 professor-attorney 2 -0.00087870070559537\n",
      "professor-attorney 3 -0.0012536750564828836\n",
      "hard  0.55 professor-attorney 3 -0.0007662557738579157\n",
      "hard  0.6 professor-attorney 3 -0.00013472567673999958\n",
      "hard  0.7 professor-attorney 3 0.00045595956844945995\n",
      "hard  0.8 professor-attorney 3 0.0005816845427372153\n",
      "hard  0.9 professor-attorney 3 0.000589651533954072\n",
      "hard  0.99 professor-attorney 3 0.0004951389488114799\n",
      "professor-attorney 4 0.0002736592940777438\n",
      "hard  0.55 professor-attorney 4 -0.00029985858861436565\n",
      "hard  0.6 professor-attorney 4 -0.00045596732076369495\n",
      "hard  0.7 professor-attorney 4 -0.0013008451628018225\n",
      "hard  0.8 professor-attorney 4 -7.269602406623221e-05\n",
      "hard  0.9 professor-attorney 4 -0.0007868095562630906\n",
      "hard  0.99 professor-attorney 4 -0.00026246069700142094\n",
      "=========================\n",
      "journalist-surgeon 0 -0.007923212056594076\n",
      "hard  0.55 journalist-surgeon 0 -0.004366999459077703\n",
      "hard  0.6 journalist-surgeon 0 -0.0007610349707554143\n",
      "hard  0.7 journalist-surgeon 0 -0.00021936735151917297\n",
      "hard  0.8 journalist-surgeon 0 -0.0016784949015240525\n",
      "hard  0.9 journalist-surgeon 0 -0.00029797783147933465\n",
      "hard  0.99 journalist-surgeon 0 -0.00688118421874917\n",
      "journalist-surgeon 1 -0.0003734488790200219\n",
      "hard  0.55 journalist-surgeon 1 -0.00017212173109659723\n",
      "hard  0.6 journalist-surgeon 1 -0.0030384417680713316\n",
      "hard  0.7 journalist-surgeon 1 -0.004776703433178375\n",
      "hard  0.8 journalist-surgeon 1 -0.007945886640178923\n",
      "hard  0.9 journalist-surgeon 1 -0.00041668783650050667\n",
      "hard  0.99 journalist-surgeon 1 -0.0025048319082833226\n",
      "journalist-surgeon 2 -7.493603015951145e-05\n",
      "hard  0.55 journalist-surgeon 2 -0.0016636102298002164\n",
      "hard  0.6 journalist-surgeon 2 -0.00122098204579979\n",
      "hard  0.7 journalist-surgeon 2 -5.584928379565035e-05\n",
      "hard  0.8 journalist-surgeon 2 -0.003051904549046247\n",
      "hard  0.9 journalist-surgeon 2 -0.0020027572413015005\n",
      "hard  0.99 journalist-surgeon 2 -0.0052798454339209044\n",
      "journalist-surgeon 3 0.004833550979327694\n",
      "hard  0.55 journalist-surgeon 3 -0.0006326144730768313\n",
      "hard  0.6 journalist-surgeon 3 0.0005632744303242232\n",
      "hard  0.7 journalist-surgeon 3 -8.016739935745587e-05\n",
      "hard  0.8 journalist-surgeon 3 -0.0010112581320651515\n",
      "hard  0.9 journalist-surgeon 3 3.3769738837508e-05\n",
      "hard  0.99 journalist-surgeon 3 -0.0015979520560638338\n",
      "journalist-surgeon 4 0.0036044968457169535\n",
      "hard  0.55 journalist-surgeon 4 -0.0008260729865037231\n",
      "hard  0.6 journalist-surgeon 4 6.943260549996388e-05\n",
      "hard  0.7 journalist-surgeon 4 7.182682011130925e-05\n",
      "hard  0.8 journalist-surgeon 4 -0.0034517025991894768\n",
      "hard  0.9 journalist-surgeon 4 -0.0015040084535516973\n",
      "hard  0.99 journalist-surgeon 4 -0.00020362664535489294\n",
      "=========================\n",
      "physician-nurse 0 -0.0019033141858902924\n",
      "hard  0.55 physician-nurse 0 -0.0010383990358482365\n",
      "hard  0.6 physician-nurse 0 -0.004834651112089139\n",
      "hard  0.7 physician-nurse 0 -0.00020189103855083879\n",
      "hard  0.8 physician-nurse 0 -0.0007210612578907316\n",
      "hard  0.9 physician-nurse 0 -8.257454921145602e-05\n",
      "hard  0.99 physician-nurse 0 6.807101160222206e-05\n",
      "physician-nurse 1 -0.005276569061493652\n",
      "hard  0.55 physician-nurse 1 -0.0017596317524739558\n",
      "hard  0.6 physician-nurse 1 0.0006526689156837184\n",
      "hard  0.7 physician-nurse 1 0.00039964165689088116\n",
      "hard  0.8 physician-nurse 1 0.00108140566663395\n",
      "hard  0.9 physician-nurse 1 0.0012445686256121746\n",
      "hard  0.99 physician-nurse 1 0.0014867503997088027\n",
      "physician-nurse 2 0.00684640140509718\n",
      "hard  0.55 physician-nurse 2 -4.138068825720875e-05\n",
      "hard  0.6 physician-nurse 2 1.9142601981991803e-05\n",
      "hard  0.7 physician-nurse 2 0.0011811232702610974\n",
      "hard  0.8 physician-nurse 2 0.0007859771932847703\n",
      "hard  0.9 physician-nurse 2 0.001033895727974632\n",
      "hard  0.99 physician-nurse 2 0.0007323940938408491\n",
      "physician-nurse 3 0.003233382952993913\n",
      "hard  0.55 physician-nurse 3 0.0007515110052758178\n",
      "hard  0.6 physician-nurse 3 0.0012744854195920752\n",
      "hard  0.7 physician-nurse 3 0.0014124917354075839\n",
      "hard  0.8 physician-nurse 3 0.0003081541437609525\n",
      "hard  0.9 physician-nurse 3 0.000890224726759925\n",
      "hard  0.99 physician-nurse 3 0.0008014916394561977\n",
      "physician-nurse 4 0.0019589432303812604\n",
      "hard  0.55 physician-nurse 4 -0.00045292753818027\n",
      "hard  0.6 physician-nurse 4 -0.0008804447032791929\n",
      "hard  0.7 physician-nurse 4 0.0005217048123123957\n",
      "hard  0.8 physician-nurse 4 0.0005129458275505705\n",
      "hard  0.9 physician-nurse 4 0.0005138249998025701\n",
      "hard  0.99 physician-nurse 4 0.00022588822940305953\n",
      "=========================\n",
      "professor-physician 0 0.000296633534659807\n",
      "hard  0.55 professor-physician 0 0.0008176236465573528\n",
      "hard  0.6 professor-physician 0 0.0001431396706935706\n",
      "hard  0.7 professor-physician 0 0.0010351158465840005\n",
      "hard  0.8 professor-physician 0 0.0009227857829312569\n",
      "hard  0.9 professor-physician 0 0.0018236965859633747\n",
      "hard  0.99 professor-physician 0 -0.0026538115310866806\n",
      "professor-physician 1 -0.0016160729992070744\n",
      "hard  0.55 professor-physician 1 -0.0012614595967956888\n",
      "hard  0.6 professor-physician 1 -0.002089215871439176\n",
      "hard  0.7 professor-physician 1 -0.0012389037961355598\n",
      "hard  0.8 professor-physician 1 -0.0009171950076861668\n",
      "hard  0.9 professor-physician 1 -0.001539391306362492\n",
      "hard  0.99 professor-physician 1 -0.0023375108315355497\n",
      "professor-physician 2 5.485740257560323e-05\n",
      "hard  0.55 professor-physician 2 -0.00063886624498799\n",
      "hard  0.6 professor-physician 2 0.0002319246065396463\n",
      "hard  0.7 professor-physician 2 -0.0010937242710864492\n",
      "hard  0.8 professor-physician 2 -5.9950027890054436e-05\n",
      "hard  0.9 professor-physician 2 -8.03040426855528e-05\n",
      "hard  0.99 professor-physician 2 0.00025065028063064876\n",
      "professor-physician 3 -0.0028628986528761446\n",
      "hard  0.55 professor-physician 3 -0.0008786967962922665\n",
      "hard  0.6 professor-physician 3 -0.004884188579725501\n",
      "hard  0.7 professor-physician 3 -0.0007372955982511975\n",
      "hard  0.8 professor-physician 3 -0.0005170422779293693\n",
      "hard  0.9 professor-physician 3 -0.0004823696248938969\n",
      "hard  0.99 professor-physician 3 -0.0006591635067517698\n",
      "professor-physician 4 -0.0008718231212055949\n",
      "hard  0.55 professor-physician 4 -0.00022224713286356224\n",
      "hard  0.6 professor-physician 4 0.0003255209281326499\n",
      "hard  0.7 professor-physician 4 0.00026484896211032005\n",
      "hard  0.8 professor-physician 4 -0.0001004700895604893\n",
      "hard  0.9 professor-physician 4 0.0007729941070914181\n",
      "hard  0.99 professor-physician 4 0.00017307837769431345\n",
      "=========================\n",
      "psychologist-teacher 0 -0.0013955922324939696\n",
      "hard  0.55 psychologist-teacher 0 -0.0014011264045173943\n",
      "hard  0.6 psychologist-teacher 0 -0.001926115921479954\n",
      "hard  0.7 psychologist-teacher 0 -0.0010776781257887391\n",
      "hard  0.8 psychologist-teacher 0 -0.0015503406630973382\n",
      "hard  0.9 psychologist-teacher 0 -0.001447213029757699\n",
      "hard  0.99 psychologist-teacher 0 -0.0030739152106156187\n",
      "psychologist-teacher 1 -0.0003365919280070573\n",
      "hard  0.55 psychologist-teacher 1 -0.00024149082080027195\n",
      "hard  0.6 psychologist-teacher 1 -0.0002251393160493942\n",
      "hard  0.7 psychologist-teacher 1 -0.0026465465855857184\n",
      "hard  0.8 psychologist-teacher 1 -0.00033905929007194135\n",
      "hard  0.9 psychologist-teacher 1 -0.002825965809944986\n",
      "hard  0.99 psychologist-teacher 1 -0.002280231379983455\n",
      "psychologist-teacher 2 -0.005399473287186818\n",
      "hard  0.55 psychologist-teacher 2 -0.000964231367572288\n",
      "hard  0.6 psychologist-teacher 2 -4.99517485614831e-05\n",
      "hard  0.7 psychologist-teacher 2 -0.0024809739791757934\n",
      "hard  0.8 psychologist-teacher 2 -0.00017037379889761617\n",
      "hard  0.9 psychologist-teacher 2 -0.0012654270607279638\n",
      "hard  0.99 psychologist-teacher 2 -9.408333727478624e-05\n",
      "psychologist-teacher 3 -0.0003928189818106498\n",
      "hard  0.55 psychologist-teacher 3 -8.659716078118773e-05\n",
      "hard  0.6 psychologist-teacher 3 -0.009105045725877892\n",
      "hard  0.7 psychologist-teacher 3 -0.002476461256785689\n",
      "hard  0.8 psychologist-teacher 3 -0.0022773690880202313\n",
      "hard  0.9 psychologist-teacher 3 -0.000919578186210579\n",
      "hard  0.99 psychologist-teacher 3 5.3346845253487984e-06\n",
      "psychologist-teacher 4 -0.005551993868439187\n",
      "hard  0.55 psychologist-teacher 4 -0.0011481849428203184\n",
      "hard  0.6 psychologist-teacher 4 -0.0059438994466650685\n",
      "hard  0.7 psychologist-teacher 4 -0.0003204586727904024\n",
      "hard  0.8 psychologist-teacher 4 -0.0015954556865367397\n",
      "hard  0.9 psychologist-teacher 4 1.286450141679829e-05\n",
      "hard  0.99 psychologist-teacher 4 2.6967183664683425e-05\n",
      "=========================\n",
      "attorney-teacher 0 -0.0015190758331422582\n",
      "hard  0.55 attorney-teacher 0 0.0019795995661562316\n",
      "hard  0.6 attorney-teacher 0 0.0015738744974971297\n",
      "hard  0.7 attorney-teacher 0 0.00174182457744132\n",
      "hard  0.8 attorney-teacher 0 0.0018486666671524432\n",
      "hard  0.9 attorney-teacher 0 0.0010482022256298462\n",
      "hard  0.99 attorney-teacher 0 0.0008995618314033837\n",
      "attorney-teacher 1 -0.00048563597315631757\n",
      "hard  0.55 attorney-teacher 1 -0.007070956421159402\n",
      "hard  0.6 attorney-teacher 1 -0.000860167873888984\n",
      "hard  0.7 attorney-teacher 1 -0.0011881335473123977\n",
      "hard  0.8 attorney-teacher 1 0.0006042567077505501\n",
      "hard  0.9 attorney-teacher 1 0.00014875938299663272\n",
      "hard  0.99 attorney-teacher 1 0.0005777792913955748\n",
      "attorney-teacher 2 7.15182618620247e-05\n",
      "hard  0.55 attorney-teacher 2 -0.0005112255134164245\n",
      "hard  0.6 attorney-teacher 2 -0.0015698375946260335\n",
      "hard  0.7 attorney-teacher 2 -0.0019335683073196108\n",
      "hard  0.8 attorney-teacher 2 -0.000907066687281799\n",
      "hard  0.9 attorney-teacher 2 -0.0003650276566633526\n",
      "hard  0.99 attorney-teacher 2 -0.005049147861973857\n",
      "attorney-teacher 3 -0.0028982252889725446\n",
      "hard  0.55 attorney-teacher 3 -0.0006475202283807713\n",
      "hard  0.6 attorney-teacher 3 -0.0004725029873791087\n",
      "hard  0.7 attorney-teacher 3 -0.0013217144358657062\n",
      "hard  0.8 attorney-teacher 3 -0.0016321286551003178\n",
      "hard  0.9 attorney-teacher 3 -0.00047864960081511754\n",
      "hard  0.99 attorney-teacher 3 -0.0006248606214271923\n",
      "attorney-teacher 4 -0.00114357632278006\n",
      "hard  0.55 attorney-teacher 4 -0.0024051912007118093\n",
      "hard  0.6 attorney-teacher 4 -0.00039937688806157556\n",
      "hard  0.7 attorney-teacher 4 0.00015195042558158178\n",
      "hard  0.8 attorney-teacher 4 0.0001537492448133726\n",
      "hard  0.9 attorney-teacher 4 -0.0006023868315137015\n",
      "hard  0.99 attorney-teacher 4 -0.0004041256284017525\n",
      "=========================\n",
      "physician-journalist 0 0.00263665411000491\n",
      "hard  0.55 physician-journalist 0 -0.002492201611111522\n",
      "hard  0.6 physician-journalist 0 -0.002453856718294234\n",
      "hard  0.7 physician-journalist 0 -0.002070029535698348\n",
      "hard  0.8 physician-journalist 0 -0.00032880060759166163\n",
      "hard  0.9 physician-journalist 0 -0.0004989480061521334\n",
      "hard  0.99 physician-journalist 0 -0.00019731059041638854\n",
      "physician-journalist 1 -0.0006610138109978125\n",
      "hard  0.55 physician-journalist 1 -0.0005663205271080329\n",
      "hard  0.6 physician-journalist 1 -0.0012462503712618034\n",
      "hard  0.7 physician-journalist 1 0.001165807217695214\n",
      "hard  0.8 physician-journalist 1 -0.003588582288670872\n",
      "hard  0.9 physician-journalist 1 0.0009929408890825275\n",
      "hard  0.99 physician-journalist 1 -0.0018054155321352017\n",
      "physician-journalist 2 -0.0031888892316852546\n",
      "hard  0.55 physician-journalist 2 9.478394330630913e-05\n",
      "hard  0.6 physician-journalist 2 -0.0002693661470488662\n",
      "hard  0.7 physician-journalist 2 -0.0014321486566983843\n",
      "hard  0.8 physician-journalist 2 7.036686076344889e-05\n",
      "hard  0.9 physician-journalist 2 0.0005766516602632166\n",
      "hard  0.99 physician-journalist 2 0.0005234626902281825\n",
      "physician-journalist 3 0.0026731901918712886\n",
      "hard  0.55 physician-journalist 3 -0.0033266718316978805\n",
      "hard  0.6 physician-journalist 3 -0.0027097122220287595\n",
      "hard  0.7 physician-journalist 3 0.0004496762550878497\n",
      "hard  0.8 physician-journalist 3 -0.0005222836472436043\n",
      "hard  0.9 physician-journalist 3 0.00012404483215444628\n",
      "hard  0.99 physician-journalist 3 -0.002279578584224784\n",
      "physician-journalist 4 -0.001164417741638446\n",
      "hard  0.55 physician-journalist 4 -0.00036001271565044846\n",
      "hard  0.6 physician-journalist 4 -0.0029240011376268082\n",
      "hard  0.7 physician-journalist 4 -0.0008786414597476977\n",
      "hard  0.8 physician-journalist 4 -0.0006378316289789021\n",
      "hard  0.9 physician-journalist 4 -0.0005468117226773428\n",
      "hard  0.99 physician-journalist 4 -5.321215008957658e-06\n",
      "=========================\n",
      "professor-dentist 0 -0.0019080609238054658\n",
      "hard  0.55 professor-dentist 0 -0.0007059280310495186\n",
      "hard  0.6 professor-dentist 0 -0.001228082168858613\n",
      "hard  0.7 professor-dentist 0 5.356438516868067e-05\n",
      "hard  0.8 professor-dentist 0 -0.0013302656949696345\n",
      "hard  0.9 professor-dentist 0 0.00035615660500520896\n",
      "hard  0.99 professor-dentist 0 0.00021405418853504\n",
      "professor-dentist 1 0.0010550748184401382\n",
      "hard  0.55 professor-dentist 1 0.00022850747282088246\n",
      "hard  0.6 professor-dentist 1 -0.003246429269467055\n",
      "hard  0.7 professor-dentist 1 0.0001270362812880732\n",
      "hard  0.8 professor-dentist 1 -2.165547669075174e-05\n",
      "hard  0.9 professor-dentist 1 6.128565380525419e-05\n",
      "hard  0.99 professor-dentist 1 -0.0009542690983690427\n",
      "professor-dentist 2 0.0020985478669256485\n",
      "hard  0.55 professor-dentist 2 -0.0016841314713843536\n",
      "hard  0.6 professor-dentist 2 0.00012897736007544403\n",
      "hard  0.7 professor-dentist 2 6.583084218969582e-05\n",
      "hard  0.8 professor-dentist 2 -0.0003688176834497625\n",
      "hard  0.9 professor-dentist 2 -0.00032830149017282206\n",
      "hard  0.99 professor-dentist 2 -0.0012658020717546403\n",
      "professor-dentist 3 0.0017930716350120779\n",
      "hard  0.55 professor-dentist 3 -0.0009126840214667142\n",
      "hard  0.6 professor-dentist 3 -0.0006710235914931806\n",
      "hard  0.7 professor-dentist 3 0.00011187071864549036\n",
      "hard  0.8 professor-dentist 3 -2.997956938177726e-05\n",
      "hard  0.9 professor-dentist 3 0.00018721921878361858\n",
      "hard  0.99 professor-dentist 3 -0.0007795925651767899\n",
      "professor-dentist 4 0.0014247885103227942\n",
      "hard  0.55 professor-dentist 4 0.000397710486385372\n",
      "hard  0.6 professor-dentist 4 0.0002017341179312293\n",
      "hard  0.7 professor-dentist 4 -0.0004569642127907336\n",
      "hard  0.8 professor-dentist 4 0.00040358328168632607\n",
      "hard  0.9 professor-dentist 4 -0.002680221428809415\n",
      "hard  0.99 professor-dentist 4 0.00012200636802983666\n",
      "=========================\n",
      "teacher-surgeon 0 -2.541144525447514e-05\n",
      "hard  0.55 teacher-surgeon 0 -0.00018174470499021655\n",
      "hard  0.6 teacher-surgeon 0 0.00013039487771848712\n",
      "hard  0.7 teacher-surgeon 0 -0.0006922493606182911\n",
      "hard  0.8 teacher-surgeon 0 -0.0005473428483905307\n",
      "hard  0.9 teacher-surgeon 0 0.00038689728033425297\n",
      "hard  0.99 teacher-surgeon 0 3.59219966878932e-05\n",
      "teacher-surgeon 1 0.0005433508550447552\n",
      "hard  0.55 teacher-surgeon 1 0.000731140039951006\n",
      "hard  0.6 teacher-surgeon 1 -5.510894525828647e-05\n",
      "hard  0.7 teacher-surgeon 1 0.0009341484094689267\n",
      "hard  0.8 teacher-surgeon 1 0.0011127063265203851\n",
      "hard  0.9 teacher-surgeon 1 0.001161626215719025\n",
      "hard  0.99 teacher-surgeon 1 -0.0028023654470036297\n",
      "teacher-surgeon 2 -0.0007122107843793302\n",
      "hard  0.55 teacher-surgeon 2 -2.886608230412424e-05\n",
      "hard  0.6 teacher-surgeon 2 -0.00044818470409346833\n",
      "hard  0.7 teacher-surgeon 2 -0.00010783380989143332\n",
      "hard  0.8 teacher-surgeon 2 -4.3220403312949074e-05\n",
      "hard  0.9 teacher-surgeon 2 -1.6659391374740373e-05\n",
      "hard  0.99 teacher-surgeon 2 -0.0006361541301821427\n",
      "teacher-surgeon 3 -0.004653724342896126\n",
      "hard  0.55 teacher-surgeon 3 0.0005341203709591236\n",
      "hard  0.6 teacher-surgeon 3 0.0006339793201274313\n",
      "hard  0.7 teacher-surgeon 3 0.0002911108626748593\n",
      "hard  0.8 teacher-surgeon 3 0.000257770656268419\n",
      "hard  0.9 teacher-surgeon 3 0.0004852892014137744\n",
      "hard  0.99 teacher-surgeon 3 0.0005547843178291822\n",
      "teacher-surgeon 4 0.0007537925102695242\n",
      "hard  0.55 teacher-surgeon 4 0.0011404357959194922\n",
      "hard  0.6 teacher-surgeon 4 0.0004711311771407045\n",
      "hard  0.7 teacher-surgeon 4 0.0010638033147908343\n",
      "hard  0.8 teacher-surgeon 4 0.001720836988777119\n",
      "hard  0.9 teacher-surgeon 4 0.0005530977757632893\n",
      "hard  0.99 teacher-surgeon 4 0.0008461040989171664\n",
      "=========================\n",
      "psychologist-surgeon 0 -0.004741535359376825\n",
      "hard  0.55 psychologist-surgeon 0 -0.002417660121059262\n",
      "hard  0.6 psychologist-surgeon 0 0.001326209788810262\n",
      "hard  0.7 psychologist-surgeon 0 -9.581818191206626e-05\n",
      "hard  0.8 psychologist-surgeon 0 0.0012734027394907255\n",
      "hard  0.9 psychologist-surgeon 0 0.000492581192637509\n",
      "hard  0.99 psychologist-surgeon 0 0.001137254025151635\n",
      "psychologist-surgeon 1 -0.001526226141615017\n",
      "hard  0.55 psychologist-surgeon 1 -0.0005215937633287204\n",
      "hard  0.6 psychologist-surgeon 1 0.0006331132184016486\n",
      "hard  0.7 psychologist-surgeon 1 0.0008701317771500072\n",
      "hard  0.8 psychologist-surgeon 1 -0.00018935919759688336\n",
      "hard  0.9 psychologist-surgeon 1 -0.0031901047369019553\n",
      "hard  0.99 psychologist-surgeon 1 0.0010974991623515784\n",
      "psychologist-surgeon 2 -0.0026689387959901723\n",
      "hard  0.55 psychologist-surgeon 2 -0.0006054805903421556\n",
      "hard  0.6 psychologist-surgeon 2 -0.00019215646878489867\n",
      "hard  0.7 psychologist-surgeon 2 -0.001460433503247649\n",
      "hard  0.8 psychologist-surgeon 2 -0.00010866986968738335\n",
      "hard  0.9 psychologist-surgeon 2 -0.0014271117115493004\n",
      "hard  0.99 psychologist-surgeon 2 -4.092674313793676e-05\n",
      "psychologist-surgeon 3 0.0007005082426932985\n",
      "hard  0.55 psychologist-surgeon 3 0.004512932325505092\n",
      "hard  0.6 psychologist-surgeon 3 0.005007648379296792\n",
      "hard  0.7 psychologist-surgeon 3 0.0031501216332299187\n",
      "hard  0.8 psychologist-surgeon 3 0.004583926526946391\n",
      "hard  0.9 psychologist-surgeon 3 0.003068265812834925\n",
      "hard  0.99 psychologist-surgeon 3 0.00301420331809632\n",
      "psychologist-surgeon 4 -0.004960340292151755\n",
      "hard  0.55 psychologist-surgeon 4 -0.0024636168198960418\n",
      "hard  0.6 psychologist-surgeon 4 -0.003328895655867914\n",
      "hard  0.7 psychologist-surgeon 4 -0.00349688754756039\n",
      "hard  0.8 psychologist-surgeon 4 -0.0026469026400118834\n",
      "hard  0.9 psychologist-surgeon 4 -0.0063292067190383605\n",
      "hard  0.99 psychologist-surgeon 4 -0.007936939423458633\n",
      "=========================\n",
      "photographer-surgeon 0 0.01745341115119592\n",
      "hard  0.55 photographer-surgeon 0 -0.0003472383072471441\n",
      "hard  0.6 photographer-surgeon 0 -0.002633847318944893\n",
      "hard  0.7 photographer-surgeon 0 -0.00444898519931447\n",
      "hard  0.8 photographer-surgeon 0 -0.0006699173375557255\n",
      "hard  0.9 photographer-surgeon 0 -0.002354844514539267\n",
      "hard  0.99 photographer-surgeon 0 0.000645838286519651\n",
      "photographer-surgeon 1 0.01697684366372032\n",
      "hard  0.55 photographer-surgeon 1 -0.00041891053242260057\n",
      "hard  0.6 photographer-surgeon 1 0.000690209140152942\n",
      "hard  0.7 photographer-surgeon 1 6.545297404192407e-05\n",
      "hard  0.8 photographer-surgeon 1 0.0009463315860980259\n",
      "hard  0.9 photographer-surgeon 1 -0.0011747949039517813\n",
      "hard  0.99 photographer-surgeon 1 -0.00046863531337648645\n",
      "photographer-surgeon 2 0.02110292848844042\n",
      "hard  0.55 photographer-surgeon 2 -0.003924696013659856\n",
      "hard  0.6 photographer-surgeon 2 0.00016399791546028997\n",
      "hard  0.7 photographer-surgeon 2 0.0004602993686383394\n",
      "hard  0.8 photographer-surgeon 2 -0.0012254278432789478\n",
      "hard  0.9 photographer-surgeon 2 -0.001442255385613378\n",
      "hard  0.99 photographer-surgeon 2 0.0009844619766126161\n",
      "photographer-surgeon 3 0.02294498833317249\n",
      "hard  0.55 photographer-surgeon 3 0.00017206752024079286\n",
      "hard  0.6 photographer-surgeon 3 0.0005262684148428232\n",
      "hard  0.7 photographer-surgeon 3 -0.00238685368700553\n",
      "hard  0.8 photographer-surgeon 3 0.0007328009534456337\n",
      "hard  0.9 photographer-surgeon 3 0.0005296486789316823\n",
      "hard  0.99 photographer-surgeon 3 -0.000608009418341271\n",
      "photographer-surgeon 4 0.01854242145262963\n",
      "hard  0.55 photographer-surgeon 4 -0.00019314831350203399\n",
      "hard  0.6 photographer-surgeon 4 0.00023771124025540047\n",
      "hard  0.7 photographer-surgeon 4 -0.0013080359962654997\n",
      "hard  0.8 photographer-surgeon 4 -0.00367531340751559\n",
      "hard  0.9 photographer-surgeon 4 -0.0012823461071174158\n",
      "hard  0.99 photographer-surgeon 4 -7.684176193234471e-06\n",
      "=========================\n",
      "attorney-psychologist 0 -0.001553941534901404\n",
      "hard  0.55 attorney-psychologist 0 -0.0009711528952162096\n",
      "hard  0.6 attorney-psychologist 0 0.0001421262238919807\n",
      "hard  0.7 attorney-psychologist 0 -0.001395585088785234\n",
      "hard  0.8 attorney-psychologist 0 -0.0005734185341391695\n",
      "hard  0.9 attorney-psychologist 0 -0.003147658016372734\n",
      "hard  0.99 attorney-psychologist 0 -0.000465575885452596\n",
      "attorney-psychologist 1 0.0015016970717495193\n",
      "hard  0.55 attorney-psychologist 1 -0.007638405114060887\n",
      "hard  0.6 attorney-psychologist 1 -0.0014290037583571946\n",
      "hard  0.7 attorney-psychologist 1 -0.006662889582140008\n",
      "hard  0.8 attorney-psychologist 1 -0.0019667309892775764\n",
      "hard  0.9 attorney-psychologist 1 0.0006078228873499514\n",
      "hard  0.99 attorney-psychologist 1 -0.00314141684493785\n",
      "attorney-psychologist 2 -0.00024888810792567106\n",
      "hard  0.55 attorney-psychologist 2 -0.00022634499201945601\n",
      "hard  0.6 attorney-psychologist 2 -0.002935243001006227\n",
      "hard  0.7 attorney-psychologist 2 -0.00201732341736105\n",
      "hard  0.8 attorney-psychologist 2 -0.0002868211317147429\n",
      "hard  0.9 attorney-psychologist 2 -0.0003336092566268434\n",
      "hard  0.99 attorney-psychologist 2 -0.0018187981250593621\n",
      "attorney-psychologist 3 -5.615211513565477e-05\n",
      "hard  0.55 attorney-psychologist 3 -0.001998052408800799\n",
      "hard  0.6 attorney-psychologist 3 -0.0009925560778815123\n",
      "hard  0.7 attorney-psychologist 3 -0.0006023241417486558\n",
      "hard  0.8 attorney-psychologist 3 -0.003399272858507052\n",
      "hard  0.9 attorney-psychologist 3 5.210591129678477e-07\n",
      "hard  0.99 attorney-psychologist 3 -0.0030633892217527547\n",
      "attorney-psychologist 4 -0.0036808179889379344\n",
      "hard  0.55 attorney-psychologist 4 -0.0023712853085298136\n",
      "hard  0.6 attorney-psychologist 4 0.00039207121772999454\n",
      "hard  0.7 attorney-psychologist 4 -0.0006900618800114167\n",
      "hard  0.8 attorney-psychologist 4 -0.0006586552198027951\n",
      "hard  0.9 attorney-psychologist 4 -0.00380415110503185\n",
      "hard  0.99 attorney-psychologist 4 -0.00027980695923568444\n",
      "=========================\n",
      "physician-teacher 0 0.000956032584400579\n",
      "hard  0.55 physician-teacher 0 4.069243929516464e-05\n",
      "hard  0.6 physician-teacher 0 -0.00041921141191458844\n",
      "hard  0.7 physician-teacher 0 -0.001083043207085188\n",
      "hard  0.8 physician-teacher 0 -0.00014298818109914624\n",
      "hard  0.9 physician-teacher 0 -0.00042761851831263353\n",
      "hard  0.99 physician-teacher 0 -0.00021214197827001957\n",
      "physician-teacher 1 -0.000560553316417467\n",
      "hard  0.55 physician-teacher 1 0.00029296278168355894\n",
      "hard  0.6 physician-teacher 1 0.00033563622767673884\n",
      "hard  0.7 physician-teacher 1 -0.0023695961790384246\n",
      "hard  0.8 physician-teacher 1 0.0002150111379964903\n",
      "hard  0.9 physician-teacher 1 0.0004961783614680915\n",
      "hard  0.99 physician-teacher 1 -0.0002999227284270489\n",
      "physician-teacher 2 0.0027532695854683986\n",
      "hard  0.55 physician-teacher 2 3.2179359030215515e-05\n",
      "hard  0.6 physician-teacher 2 0.0016281892413845256\n",
      "hard  0.7 physician-teacher 2 0.0019845340425743707\n",
      "hard  0.8 physician-teacher 2 0.0010038880478426826\n",
      "hard  0.9 physician-teacher 2 0.0023379476692994006\n",
      "hard  0.99 physician-teacher 2 0.00168239760414024\n",
      "physician-teacher 3 -0.003719011444146769\n",
      "hard  0.55 physician-teacher 3 0.0008999529969052622\n",
      "hard  0.6 physician-teacher 3 0.0010383650288237067\n",
      "hard  0.7 physician-teacher 3 3.151935399980932e-05\n",
      "hard  0.8 physician-teacher 3 0.0013342354712172488\n",
      "hard  0.9 physician-teacher 3 0.0004416411245293794\n",
      "hard  0.99 physician-teacher 3 0.0008287432047688892\n",
      "physician-teacher 4 -0.001599517661165839\n",
      "hard  0.55 physician-teacher 4 -0.0028070516021317937\n",
      "hard  0.6 physician-teacher 4 -0.003178065687400533\n",
      "hard  0.7 physician-teacher 4 -0.004581556442799228\n",
      "hard  0.8 physician-teacher 4 -0.0035511282823688095\n",
      "hard  0.9 physician-teacher 4 -0.0022777767480848254\n",
      "hard  0.99 physician-teacher 4 -0.0030947724693030088\n",
      "=========================\n",
      "professor-teacher 0 0.0011172522000333363\n",
      "hard  0.55 professor-teacher 0 0.000634742730050708\n",
      "hard  0.6 professor-teacher 0 -0.0003017514511992614\n",
      "hard  0.7 professor-teacher 0 -0.0005344683889703106\n",
      "hard  0.8 professor-teacher 0 -0.0009092201181560844\n",
      "hard  0.9 professor-teacher 0 0.0007189653908243887\n",
      "hard  0.99 professor-teacher 0 -0.0011776930926175044\n",
      "professor-teacher 1 0.0011202712146262117\n",
      "hard  0.55 professor-teacher 1 0.0012040664050161354\n",
      "hard  0.6 professor-teacher 1 0.0012620226000973345\n",
      "hard  0.7 professor-teacher 1 0.0016334299087502613\n",
      "hard  0.8 professor-teacher 1 0.0009084145877570338\n",
      "hard  0.9 professor-teacher 1 0.001958734617965363\n",
      "hard  0.99 professor-teacher 1 0.00167855553239038\n",
      "professor-teacher 2 0.0009784156616139406\n",
      "hard  0.55 professor-teacher 2 0.00158538873852343\n",
      "hard  0.6 professor-teacher 2 0.00013698010107343617\n",
      "hard  0.7 professor-teacher 2 0.0009769782846158481\n",
      "hard  0.8 professor-teacher 2 0.001772637095100782\n",
      "hard  0.9 professor-teacher 2 0.0010432288536625922\n",
      "hard  0.99 professor-teacher 2 0.0018482210585907266\n",
      "professor-teacher 3 0.0004988440147017936\n",
      "hard  0.55 professor-teacher 3 -0.000368968739354103\n",
      "hard  0.6 professor-teacher 3 -0.0007919002717702295\n",
      "hard  0.7 professor-teacher 3 -0.0008315826018894246\n",
      "hard  0.8 professor-teacher 3 0.000185526321961893\n",
      "hard  0.9 professor-teacher 3 -0.0018935679686361562\n",
      "hard  0.99 professor-teacher 3 0.001292905379112974\n",
      "professor-teacher 4 0.0017685375836482553\n",
      "hard  0.55 professor-teacher 4 0.00020497252617446282\n",
      "hard  0.6 professor-teacher 4 0.00012789277963820567\n",
      "hard  0.7 professor-teacher 4 0.0002874247777293615\n",
      "hard  0.8 professor-teacher 4 0.00030819747391475083\n",
      "hard  0.9 professor-teacher 4 -0.00038554280298175314\n",
      "hard  0.99 professor-teacher 4 -0.0010177289849110371\n",
      "=========================\n",
      "professor-psychologist 0 -0.00042791152312815495\n",
      "hard  0.55 professor-psychologist 0 -0.0011287109692154074\n",
      "hard  0.6 professor-psychologist 0 -0.0016907703613679992\n",
      "hard  0.7 professor-psychologist 0 -0.00017937701700165842\n",
      "hard  0.8 professor-psychologist 0 -0.00023315713317229037\n",
      "hard  0.9 professor-psychologist 0 -0.0004320925779748297\n",
      "hard  0.99 professor-psychologist 0 -0.00109367853977127\n",
      "professor-psychologist 1 0.0014495878028986109\n",
      "hard  0.55 professor-psychologist 1 0.00029473147133873834\n",
      "hard  0.6 professor-psychologist 1 -0.0016156629424762814\n",
      "hard  0.7 professor-psychologist 1 0.0003596928664068022\n",
      "hard  0.8 professor-psychologist 1 0.00026512171049253297\n",
      "hard  0.9 professor-psychologist 1 -0.004204815867980494\n",
      "hard  0.99 professor-psychologist 1 7.968667773439186e-05\n",
      "professor-psychologist 2 -3.838674617062665e-06\n",
      "hard  0.55 professor-psychologist 2 4.120528603424045e-05\n",
      "hard  0.6 professor-psychologist 2 -0.0006730875526399505\n",
      "hard  0.7 professor-psychologist 2 -0.00012005894013511487\n",
      "hard  0.8 professor-psychologist 2 -0.00016206901385340533\n",
      "hard  0.9 professor-psychologist 2 -0.0014578731601915251\n",
      "hard  0.99 professor-psychologist 2 -0.0004342624924422722\n",
      "professor-psychologist 3 -0.0008639305362058147\n",
      "hard  0.55 professor-psychologist 3 -0.0013949832391726424\n",
      "hard  0.6 professor-psychologist 3 -0.0027477860470950244\n",
      "hard  0.7 professor-psychologist 3 -0.00017652588415573156\n",
      "hard  0.8 professor-psychologist 3 -0.0005999092909592108\n",
      "hard  0.9 professor-psychologist 3 -0.0005659628257482785\n",
      "hard  0.99 professor-psychologist 3 -0.003878571562735189\n",
      "professor-psychologist 4 -0.0013455625916425351\n",
      "hard  0.55 professor-psychologist 4 -0.00014114131398612617\n",
      "hard  0.6 professor-psychologist 4 -0.0013293152710200928\n",
      "hard  0.7 professor-psychologist 4 -0.0019393206399846985\n",
      "hard  0.8 professor-psychologist 4 -0.000659032953126637\n",
      "hard  0.9 professor-psychologist 4 -0.0005959294173903951\n",
      "hard  0.99 professor-psychologist 4 -0.0016253360342158851\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "num_clfs = 5\n",
    "do_proj=True\n",
    "epsilons = [0.55, 0.6,0.7,0.8,0.9,0.99]\n",
    "pair2vinfo_soft_before = defaultdict(dict)\n",
    "pair2vinfo_hard_before = defaultdict(dict)\n",
    "pair2vinfo_soft_after = defaultdict(dict)\n",
    "pair2vinfo_hard_after = defaultdict(dict)\n",
    "\n",
    "\n",
    "for pair in pairs_sample[:]:\n",
    "    \n",
    "        with open(\"acl/data/{}-{}.pickle\".format(pair[0], pair[1]), \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            train_x, train_z, train_y = data[\"train\"]\n",
    "            dev_x, dev_z, dev_y = data[\"dev\"]\n",
    "        \n",
    "        if do_proj:\n",
    "            with open(\"acl/projs/{}-{}.pickle\".format(pair[0], pair[1]), \"rb\") as f:\n",
    "                P = pickle.load(f)[\"P\"]\n",
    "            \n",
    "        train_x_proj = train_x@P\n",
    "        dev_x_proj = dev_x@P\n",
    "        pair_str = pair[0]+ \"-\" + pair[1]\n",
    "        entropy = get_entropy(dev_z)\n",
    "        \n",
    "        for i in range(num_clfs):\n",
    "            prof_clf =  load_prof_clf(pair, ind=i)\n",
    "            probs_train = prof_clf.predict_proba(train_x_proj)\n",
    "            probs_dev = prof_clf.predict_proba(dev_x_proj)\n",
    "            \n",
    "            gender_clf = init_classifier()\n",
    "            gender_clf.fit(probs_train, train_z)\n",
    "            loss = sklearn.metrics.log_loss(dev_z, gender_clf.predict_proba(probs_dev))\n",
    "            print(pair_str, i, entropy - loss)\n",
    "            \n",
    "            pair2vinfo_hard_before[pair_str][i] = dict()\n",
    "            \n",
    "            for eps in epsilons:\n",
    "                \n",
    "                hard_labels_train = np.array([[1-eps,eps] if p[1] > 0.5 else [eps,1-eps] for p in probs_train])\n",
    "                hard_labels_dev = np.array([[1-eps,eps] if p[1] > 0.5 else [eps,1-eps] for p in probs_dev])\n",
    "                gender_clf = init_classifier()\n",
    "                gender_clf.fit(hard_labels_train, train_z)\n",
    "                loss = sklearn.metrics.log_loss(dev_z, gender_clf.predict_proba(hard_labels_dev))\n",
    "                print(\"hard \", eps, pair_str, i,  entropy - loss)\n",
    "                #pair2vinfo_hard[pair_str][i][cutoff] = entropy - loss      \n",
    "        print(\"=========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train composition classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_hidden_neuron_net(X, z, nonlinearity = True, hidden_neurons = 2, sigmoid=False, batch_size=2048,\n",
    "                                  verbose=False):\n",
    "\n",
    "    # create an MLP with a single sigmoid neuron\n",
    "    \n",
    "    if nonlinearity:\n",
    "        if sigmoid:\n",
    "            net = torch.nn.Sequential(torch.nn.Linear(X.shape[1], 1,bias=False), torch.nn.Sigmoid(),\n",
    "                             torch.nn.Linear(1, 1, bias=True))\n",
    "        else:\n",
    "            net = torch.nn.Sequential(torch.nn.Linear(X.shape[1], hidden_neurons), torch.nn.Softmax(dim=1),\n",
    "                             torch.nn.Linear(hidden_neurons, 1))\n",
    "    else:\n",
    "        net = torch.nn.Sequential(torch.nn.Linear(X.shape[1], 1), torch.nn.Linear(1,1))\n",
    "        \n",
    "    X_torch = torch.tensor(X).float()\n",
    "    z_torch = torch.tensor(z).float()\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for t in range(25000):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_indices = torch.randint(0, X.shape[0], (batch_size,))\n",
    "        z_pred_batch = net(X_torch[batch_indices])\n",
    "        z_batch = z_torch[batch_indices]        \n",
    "        loss = loss_fn(z_pred_batch.squeeze(), z_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if t % 2000 == 0 and verbose:\n",
    "            print(loss.detach().cpu().numpy().item())\n",
    "        \n",
    "    return net, loss.detach().cpu().numpy().item()\n",
    "\n",
    "def get_loss_value(net, X, z):\n",
    "    X_torch = torch.tensor(X).float()\n",
    "    z_torch = torch.tensor(z).int()\n",
    "    y_pred = net(X_torch)\n",
    "    loss = torch.nn.BCEWithLogitsLoss()(y_pred, z_torch)\n",
    "    return loss.item().detach().cpu().numpy()\n",
    "\n",
    "def get_classification_accuracy(net, X, z):\n",
    "    # calculate the accuracy of the sigmoid network\n",
    "\n",
    "    X_torch = torch.tensor(X).float()\n",
    "    z_torch = torch.tensor(z).int()\n",
    "    y_pred = (net(X_torch) > 0).int()\n",
    "    acc = (y_pred.detach().cpu().squeeze().int().numpy() == z).astype(int).mean()\n",
    "    return acc\n",
    "\n",
    "def build_equivalent_linear_model(net,X):\n",
    "    \n",
    "    if net[0].weight.shape[0] == 2:\n",
    "        alpha = (net[0].weight[0]-net[0].weight[1]).detach().clone().detach().cpu().numpy()\n",
    "        print(net[2].weight)\n",
    "        beta = (net[2].weight[0][0]-net[0].weight[0][1]).detach().clone().detach().cpu().numpy()\n",
    "    else:\n",
    "        alpha = net[0].weight.detach().clone().detach().cpu().numpy()\n",
    "        beta = net[2].weight.detach().clone().detach().cpu().numpy()\n",
    "        \n",
    "    threshold = 0.5\n",
    "    \n",
    "    gamma = net[2].bias.detach().clone().detach().cpu().numpy()\n",
    "    \n",
    "    model = torch.nn.Linear(X.shape[1], 1)\n",
    "    model.weight.data = torch.tensor(alpha).float()\n",
    "    D = -(np.log((1-threshold)/threshold) + gamma)/beta\n",
    "    new_threshold = torch.tensor(-np.log((1-D)/D)).float()\n",
    "    model.bias.data = -new_threshold \n",
    "    return model\n",
    "\n",
    "with open(\"acl/interim/pair2genderclf.pickle\", \"rb\") as f:\n",
    "    pair2genderclf = pickle.load(f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c1b63d7e45613d02ba95dbc9593569dea1cc5a68834797d07b467a998ca56ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
